{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, sqrt, pow\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"app\").getOrCreate()\n",
    "\n",
    "weather_dataset_paths = [\n",
    "    \"out_2017.txt\",\n",
    "    \"out_2018.txt\",\n",
    "    \"out_2019.txt\",\n",
    "    \"out_2020.txt\"\n",
    "]\n",
    "\n",
    "stocks_dataset_path = \"MS1.txt\"\n",
    "\n",
    "number_stocks_and_weather = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weather_transforms(path):\n",
    "    # Read data\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"dateFormat\", \"yyyy-mm-dd\") \\\n",
    "        .csv(path)\n",
    "    # Split temperature into variables\n",
    "    df = df.withColumn('TEMP', f.split(df['TMP'], ' ').getItem(0)) \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(\n",
    "        \"splitcount\", \"LOCATION\", \"WIND\", \"TMP\", \"DEW\", \"SLP\", \"tmp_quality\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+------------+------------+----------+-------------+\n",
      "| STATION_ID|      DATE|  TEMP|readvalue_ff|readvalue_bf|      unix|TEMP_INTERPOL|\n",
      "+-----------+----------+------+------------+------------+----------+-------------+\n",
      "|70259526559|2017-01-01| -50.0|       -50.0|       -50.0|1483225200|        -50.0|\n",
      "|70259526559|2017-01-02|-130.0|      -130.0|      -130.0|1483311600|       -130.0|\n",
      "|70259526559|2017-01-03|-130.0|      -130.0|      -130.0|1483398000|       -130.0|\n",
      "|70259526559|2017-01-04|-150.0|      -150.0|      -150.0|1483484400|       -150.0|\n",
      "|70259526559|2017-01-05|-130.0|      -130.0|      -130.0|1483570800|       -130.0|\n",
      "|70259526559|2017-01-06|-120.0|      -120.0|      -120.0|1483657200|       -120.0|\n",
      "|70259526559|2017-01-07|-170.0|      -170.0|      -170.0|1483743600|       -170.0|\n",
      "|70259526559|2017-01-08|-160.0|      -160.0|      -160.0|1483830000|       -160.0|\n",
      "|70259526559|2017-01-09|-140.0|      -140.0|      -140.0|1483916400|       -140.0|\n",
      "|70259526559|2017-01-10|-120.0|      -120.0|      -120.0|1484002800|       -120.0|\n",
      "+-----------+----------+------+------------+------------+----------+-------------+\n",
      "\n",
      "+-----------+----------+----+------------+------------+----------+-------------+\n",
      "| STATION_ID|      DATE|TEMP|readvalue_ff|readvalue_bf|      unix|TEMP_INTERPOL|\n",
      "+-----------+----------+----+------------+------------+----------+-------------+\n",
      "|70259526559|2017-05-02|null|        80.0|       128.0|1493676000|        104.0|\n",
      "|70259526559|2017-05-03|null|        80.0|       128.0|1493762400|        104.0|\n",
      "|70259526559|2017-06-23|null|       150.0|       100.0|1498168800|        125.0|\n",
      "|70259526559|2018-02-03|null|       -78.0|       -61.0|1517612400|        -69.5|\n",
      "|70259526559|2018-02-22|null|       -11.0|       -83.0|1519254000|        -47.0|\n",
      "|70259526559|2019-10-13|null|        78.0|       -10.0|1570917600|         34.0|\n",
      "|70259526559|2019-10-14|null|        78.0|       -10.0|1571004000|         34.0|\n",
      "|71453099999|2017-09-20|null|       192.0|       159.0|1505858400|        175.5|\n",
      "|71453099999|2018-11-17|null|        16.0|        96.0|1542409200|         56.0|\n",
      "|71453099999|2018-11-18|null|        16.0|        96.0|1542495600|         56.0|\n",
      "+-----------+----------+----+------------+------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import sys\n",
    "\n",
    "# Load and concatenate all weather data\n",
    "union = weather_transforms(weather_dataset_paths[0])\n",
    "for weather_dataset_path in weather_dataset_paths[1:]:\n",
    "    union = union.union(weather_transforms(weather_dataset_path))\n",
    "\n",
    "# Select stations which easy start date and then first 1000 of those\n",
    "weather_df = union.filter(union.DATE == \"2017-01-01\") \\\n",
    "    .select('STATION_ID') \\\n",
    "    .limit(number_stocks_and_weather)\n",
    "\n",
    "# Filter these over the complete dataset\n",
    "filtered_union = weather_df.join(union, 'STATION_ID')\n",
    "\n",
    "# Fill the easy start stations with min and max date\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')) \\\n",
    "    .withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "\n",
    "# Expand to add all days\n",
    "weather_df = weather_df \\\n",
    "    .withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))) \\\n",
    "    .drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# Left join to make sure all dates are there (missing dates will get null)\n",
    "weather_df = weather_df \\\n",
    "    .join(filtered_union, [\"STATION_ID\", \"DATE\"], \"left\") \\\n",
    "    .sort(['STATION_ID', 'DATE'])\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"TEMP\" , weather_df.TEMP.cast('float'))\n",
    "weather_df = weather_df.replace([999, 9999], None)\n",
    "\n",
    "#weather_df = weather_df.na.fill(20) # TODO\n",
    "\n",
    "# window_ff = Window.partitionBy('STATION_ID')\\\n",
    "#                .orderBy('DATE')\\\n",
    "#                .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "window_ff = Window.partitionBy('STATION_ID')\\\n",
    "               .orderBy('DATE')\\\n",
    "               .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "window_bf = Window.partitionBy('STATION_ID')\\\n",
    "               .orderBy('DATE')\\\n",
    "               .rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "               \n",
    "               \n",
    "# window_bf = Window.partitionBy('STATION_ID')\\\n",
    "#                .orderBy('DATE')\\\n",
    "#                .rowsBetween(0, sys.maxsize)\n",
    "        \n",
    "def interpol(x, x_prev, x_next, y_prev, y_next, y):\n",
    "    # return FloatType(2.0)\n",
    "    if x_prev == x_next:\n",
    "        return y\n",
    "    else:\n",
    "        m = (y_next-y_prev)/(x_next-x_prev)\n",
    "        y_interpol = y_prev + m * (x - x_prev)\n",
    "        return y_interpol\n",
    "\n",
    "def interpol_middle(y, y_prev, y_next):\n",
    "    if y == y_prev:\n",
    "        return y\n",
    "    else:\n",
    "        return (y_prev + y_next) / 2.0\n",
    "\n",
    "#Forward and backwards filling, takes around 6 minutes to complete\n",
    "read_last = f.last(weather_df['TEMP'], ignorenulls=True).over(window_ff)\n",
    "readtime_last = f.last(weather_df['DATE'], ignorenulls=True).over(window_ff)\n",
    "\n",
    "read_next = f.first(weather_df['TEMP'], ignorenulls=True).over(window_bf)\n",
    "readtime_next = f.first(weather_df['DATE'], ignorenulls=True).over(window_bf)\n",
    "\n",
    "# add the columns to the dataframe\n",
    "df_filled = weather_df.withColumn('readvalue_ff', read_last.cast('float'))\\\n",
    "                        .withColumn('readtime_ff', readtime_last)\\\n",
    "                        .withColumn('readvalue_bf', read_next.cast('float'))\\\n",
    "                        .withColumn('readtime_bf', readtime_next)\n",
    "            \n",
    "interpol_udf = f.udf(interpol, FloatType())   \n",
    "interpol_middle_udf = f.udf(interpol_middle, FloatType())\n",
    "## Set Date To Unix Time\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "df_filled = df_filled.withColumn('unix', unix_timestamp('DATE'))\n",
    "\n",
    "\n",
    "# df_filled = df_filled.withColumn('TEMP_INTERPOL', interpol_udf('unix', 'readtime_ff', 'readtime_bf', 'readvalue_ff', 'readvalue_bf', 'TEMP'))\\\n",
    "#                     .drop('readtime_ff', 'readtime_bf')\\\n",
    "\n",
    "df_filled = df_filled.withColumn('TEMP_INTERPOL', interpol_middle_udf('TEMP','readvalue_ff', 'readvalue_bf'))\\\n",
    "                    .drop('readtime_ff', 'readtime_bf')\\\n",
    "\n",
    "# UNCOMMENT\n",
    "df_filled.persist()\n",
    "\n",
    "# df_filled.schema\n",
    "df_filled.limit(10).show()\n",
    "# print(f\"Number of rows: {weather_df.count()}\")\n",
    "df_filled.filter(df_filled.TEMP.isNull()).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|               STOCK|      DATE|PRICE|\n",
      "+--------------------+----------+-----+\n",
      "|12173.Asien--Aust...|2017-01-01|   20|\n",
      "|12173.Asien--Aust...|2017-01-02|20201|\n",
      "|12173.Asien--Aust...|2017-01-03|20499|\n",
      "|12173.Asien--Aust...|2017-01-04|19780|\n",
      "|12173.Asien--Aust...|2017-01-05|19550|\n",
      "|12173.Asien--Aust...|2017-01-06|19999|\n",
      "|12173.Asien--Aust...|2017-01-07|   20|\n",
      "|12173.Asien--Aust...|2017-01-08|   20|\n",
      "|12173.Asien--Aust...|2017-01-09|19600|\n",
      "|12173.Asien--Aust...|2017-01-10|19825|\n",
      "|12173.Asien--Aust...|2017-01-11|19215|\n",
      "|12173.Asien--Aust...|2017-01-12|19501|\n",
      "|12173.Asien--Aust...|2017-01-13|19397|\n",
      "|12173.Asien--Aust...|2017-01-14|   20|\n",
      "|12173.Asien--Aust...|2017-01-15|   20|\n",
      "|12173.Asien--Aust...|2017-01-16|19329|\n",
      "|12173.Asien--Aust...|2017-01-17|19674|\n",
      "|12173.Asien--Aust...|2017-01-18|19900|\n",
      "|12173.Asien--Aust...|2017-01-19|19850|\n",
      "|12173.Asien--Aust...|2017-01-20|19808|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 21900\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "stocks_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .csv(stocks_dataset_path)\n",
    "\n",
    "# Rename columns and drop volume\n",
    "stocks_df = stocks_df.selectExpr(\n",
    "    '_c0 AS STOCK',\n",
    "    '_c1 AS DATE',\n",
    "    '_c2 AS PRICE',\n",
    "    '_c3 AS VOLUME'\n",
    ")\n",
    "stocks_df = stocks_df.drop(\"VOLUME\")\n",
    "\n",
    "# Select 1000 stocks with appropriate years\n",
    "df_stocks_selection = stocks_df \\\n",
    "    .filter((stocks_df.DATE.contains(\"2017\") | stocks_df.DATE.contains(\"2018\") | stocks_df.DATE.contains(\"2019\")))\n",
    "df_stocks_selection_filtered = df_stocks_selection \\\n",
    "    .filter(df_stocks_selection.DATE == \"01/02/2017\") \\\n",
    "    .select('STOCK') \\\n",
    "    .limit(number_stocks_and_weather)\n",
    "selected_stocks = df_stocks_selection_filtered \\\n",
    "    .join(df_stocks_selection, 'STOCK')\n",
    "\n",
    "# \n",
    "stocks_df = df_stocks_selection_filtered \\\n",
    "    .withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')) \\\n",
    "    .withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "stocks_df = stocks_df \\\n",
    "    .withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))) \\\n",
    "    .drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# \n",
    "modifiedDF = selected_stocks \\\n",
    "    .withColumn(\"DATE\", f.to_date(\"DATE\", \"MM/dd/yyyy\")) \\\n",
    "    .dropDuplicates([\"STOCK\", \"DATE\"])\n",
    "stocks_df = stocks_df \\\n",
    "    .join(modifiedDF, [\"Stock\", \"DATE\"], \"left\") \\\n",
    "    .sort(['STOCK', 'DATE'])\n",
    "\n",
    "stocks_df = stocks_df.withColumn(\"PRICE\", stocks_df.PRICE.cast(\"int\"))\n",
    "\n",
    "#Forward and backwards filling, takes around 6 minutes to complete\n",
    "w_forward = Window.partitionBy().orderBy('Stock').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "w_backward = Window.partitionBy().orderBy('Stock').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "w_forward2 = Window.partitionBy().orderBy('Stock').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "w_backward2 = Window.partitionBy().orderBy('Stock').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "stocks_df = stocks_df.withColumn('Price',f.last('Price',ignorenulls=True).over(w_forward))\\\n",
    "                             .withColumn('Volume',f.last('Volume',ignorenulls=True).over(w_forward2))\\\n",
    "                             .withColumn('Price',f.first('Price',ignorenulls=True).over(w_backward))\\\n",
    "                             .withColumn('Volume',f.first('Volume',ignorenulls=True).over(w_backward2))\n",
    "                             \n",
    "\n",
    "stocks_df.persist()\n",
    "\n",
    "stocks_df.show()\n",
    "print(f\"Number of rows: {stocks_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SQL query used:\n",
    "\n",
    "SELECT sum(X*Y') / (sqrt(sum(X))*sqrt(sum(Y')))\n",
    "FROM (\n",
    "    SELECT S.DATE, S.STOCK, W.STATION_ID_1, W.STATION_ID_2, S.PRICE as X, W.Y'\n",
    "    FROM stocks AS S, (\n",
    "        SELECT W1.DATE, W1.STATION_ID AS STATION_ID_1, W2.STATION_ID AS STATION_ID_2, avg|max|min(W1.TEMP, W2.TEMP) as W.Y'\n",
    "        FROM weather AS W1, weather AS W2,\n",
    "        WHERE W1.DATE = W2.DATE\n",
    "    ) AS W\n",
    "    WHERE S.DATE = W.DATE\n",
    ")\n",
    "GROUPBY STOCK, STATION_ID_1, STATION_ID_2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+------+\n",
      "|      DATE|STATION_ID_1|STATION_ID_2|    Y'|\n",
      "+----------+------------+------------+------+\n",
      "|2017-01-01| 02265099999| 94804099999|  85.0|\n",
      "|2017-01-01| 02265099999| 94677099999|  95.5|\n",
      "|2017-01-01| 02265099999| 94584099999| 156.5|\n",
      "|2017-01-01| 02265099999| 83650099999| 127.5|\n",
      "|2017-01-01| 02265099999| 76749399999| 137.5|\n",
      "|2017-01-01| 02265099999| 71889099999| -22.0|\n",
      "|2017-01-01| 02265099999| 71453099999| -20.0|\n",
      "|2017-01-01| 02265099999| 71450099999| -37.0|\n",
      "|2017-01-01| 02265099999| 71322099999|-171.5|\n",
      "|2017-01-01| 02265099999| 70333325518|  31.0|\n",
      "|2017-01-01| 02265099999| 70259526559| -27.5|\n",
      "|2017-01-01| 02265099999| 68487099999|  90.0|\n",
      "|2017-01-01| 02265099999| 63708099999| 112.5|\n",
      "|2017-01-01| 02265099999| 62398099999|  37.5|\n",
      "|2017-01-01| 02265099999| 54292099999| -76.0|\n",
      "|2017-01-01| 02265099999| 43278099999| 115.5|\n",
      "|2017-01-01| 02265099999| 41862199999|  63.0|\n",
      "|2017-01-01| 02265099999| 28552099999| -17.0|\n",
      "|2017-01-01| 02265099999| 06022499999|  32.5|\n",
      "|2017-01-01| 02265099999| 02265099999|  -5.0|\n",
      "+----------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 438000\n"
     ]
    }
   ],
   "source": [
    "# agg_function = f.greatest\n",
    "# agg_function = f.least\n",
    "agg_function = lambda col1, col2: (col(col1) + col(col2)) / 2\n",
    "\n",
    "Y_df = weather_df \\\n",
    "    .withColumnRenamed(\"STATION_ID\", \"STATION_ID_1\") \\\n",
    "    .withColumnRenamed(\"TEMP\", \"TEMP_1\") \\\n",
    "    .join(\n",
    "        weather_df \\\n",
    "            .withColumnRenamed(\"STATION_ID\", \"STATION_ID_2\") \\\n",
    "            .withColumnRenamed(\"TEMP\", \"TEMP_2\"), \n",
    "        \"DATE\"\n",
    "    )\n",
    "Y_df = Y_df.withColumn(\"Y'\", agg_function(\"TEMP_1\", \"TEMP_2\"))\n",
    "Y_df = Y_df.drop(\"TEMP_1\", \"TEMP_2\")\n",
    "Y_df.persist()\n",
    "Y_df.show()\n",
    "print(f\"Number of rows: {Y_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+-------------------+\n",
      "|               STOCK|STATION_ID_1|STATION_ID_2|         COSINE_SIM|\n",
      "+--------------------+------------+------------+-------------------+\n",
      "|42814.Nordamerika...| 02265099999| 94677099999| 0.8642378548973022|\n",
      "|4438.Asien--Austr...| 02265099999| 76749399999| 0.7725067131938536|\n",
      "|43297.Nordamerika...| 02265099999| 43278099999| 0.8487977948316912|\n",
      "|13784.Europa_Deut...| 06022499999| 94677099999| 0.5193424493119109|\n",
      "|28549.Futures--In...| 28552099999| 94804099999| 0.7120823821302918|\n",
      "|32843.Nordamerika...| 28552099999| 62398099999| 0.6929047123707867|\n",
      "|31474.Nordamerika...| 28552099999| 54292099999|0.11055468034407347|\n",
      "|41574.Nordamerika...| 41862199999| 71450099999|0.45457949948720755|\n",
      "|34271.Nordamerika...| 43278099999| 94584099999| 0.5726049863036856|\n",
      "|13784.Europa_Deut...| 43278099999| 71453099999| 0.4919507413036301|\n",
      "|38565.Nordamerika...| 62398099999| 71450099999| 0.6591657325739723|\n",
      "|37239.Nordamerika...| 68487099999| 02265099999| 0.7869850544965445|\n",
      "|4438.Asien--Austr...| 70259526559| 43278099999| 0.7717232453680575|\n",
      "|13784.Europa_Deut...| 70259526559| 41862199999| 0.4370994506051258|\n",
      "|40528.Nordamerika...| 70333325518| 94804099999| 0.5286089090038335|\n",
      "|4438.Asien--Austr...| 70333325518| 71450099999| 0.4419847306754703|\n",
      "|41574.Nordamerika...| 70333325518| 06022499999| 0.5116370945591366|\n",
      "|40702.Nordamerika...| 70333325518| 06022499999| 0.8788932729977597|\n",
      "|13784.Europa_Deut...| 71322099999| 83650099999| 0.3997707207427156|\n",
      "|28721.Futures--In...| 71322099999| 71889099999|0.16830833044206067|\n",
      "+--------------------+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 8000\n"
     ]
    }
   ],
   "source": [
    "combined_df = stocks_df \\\n",
    "    .withColumnRenamed(\"PRICE\", \"X\") \\\n",
    "    .join(Y_df, \"DATE\")\n",
    "\n",
    "combined_df_grouped = combined_df \\\n",
    "    .groupBy(\"STOCK\", \"STATION_ID_1\", \"STATION_ID_2\") \\\n",
    "    .agg(\n",
    "        f.sqrt(f.sum(f.pow(\"X\", 2))).alias(\"X_norm\"),\n",
    "        f.sqrt(f.sum(f.pow(\"Y'\", 2))).alias(\"Y_norm\"),\n",
    "        f.sum(col(\"X\") * col(\"Y'\")).alias(\"XY\")\n",
    "    )\n",
    "\n",
    "combined_df_grouped = combined_df_grouped \\\n",
    "    .withColumn(\"COSINE_SIM\", col(\"XY\") / (col(\"Y_norm\") * col(\"X_norm\")))\n",
    "combined_df_grouped = combined_df_grouped.drop(\"X_norm\", \"Y_norm\", \"XY\")\n",
    "combined_df_grouped.persist()\n",
    "combined_df_grouped.show()\n",
    "\n",
    "print(f\"Number of rows: {combined_df_grouped.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stocks_df.filter(col(\"PRICE\").isNotNull()).count()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3057157393a05f1e3a652ff8b92c3f8767e072f6c4cd1c84e087da81568d2ff"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
