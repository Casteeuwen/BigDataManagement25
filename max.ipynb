{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, sqrt, pow\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"app\").getOrCreate()\n",
    "\n",
    "weather_dataset_paths = [\n",
    "    \"out_2017.txt\",\n",
    "    \"out_2018.txt\",\n",
    "    \"out_2019.txt\",\n",
    "    \"out_2020.txt\"\n",
    "]\n",
    "\n",
    "stocks_dataset_path = \"MS1.txt\"\n",
    "\n",
    "number_stocks_and_weather = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_transforms(path):\n",
    "    # Read data\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"dateFormat\", \"yyyy-mm-dd\") \\\n",
    "        .csv(path)\n",
    "    # Split temperature into variables\n",
    "    df = df.withColumn('TEMP', f.split(df['TMP'], ' ').getItem(0)) \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(\n",
    "        \"splitcount\", \"LOCATION\", \"WIND\", \"TMP\", \"DEW\", \"SLP\", \"tmp_quality\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1000\n",
      "+-----------+----------+-----+\n",
      "| STATION_ID|      DATE| TEMP|\n",
      "+-----------+----------+-----+\n",
      "|01367099999|2017-01-01| -0.8|\n",
      "|01367099999|2017-01-02| -2.8|\n",
      "|01367099999|2017-01-03| -6.6|\n",
      "|01367099999|2017-01-04| -7.9|\n",
      "|01367099999|2017-01-05|-13.7|\n",
      "|01367099999|2017-01-06|-15.1|\n",
      "|01367099999|2017-01-07| -9.1|\n",
      "|01367099999|2017-01-08| -9.2|\n",
      "|01367099999|2017-01-09| -4.8|\n",
      "|01367099999|2017-01-10|  1.9|\n",
      "+-----------+----------+-----+\n",
      "\n",
      "+----------+----+----+\n",
      "|STATION_ID|DATE|TEMP|\n",
      "+----------+----+----+\n",
      "+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import sys\n",
    "\n",
    "# Load and concatenate all weather data\n",
    "union = weather_transforms(weather_dataset_paths[0])\n",
    "for weather_dataset_path in weather_dataset_paths[1:]:\n",
    "    union = union.union(weather_transforms(weather_dataset_path))\n",
    "\n",
    "# Select stations which easy start date and then first 1000 of those\n",
    "weather_df = union.filter(union.DATE == \"2017-01-01\") \\\n",
    "    .select('STATION_ID') \\\n",
    "    .limit(number_stocks_and_weather)\n",
    "\n",
    "print(f\"Number of rows: {weather_df.count()}\")\n",
    "\n",
    "# Filter these over the complete dataset\n",
    "filtered_union = weather_df.join(union, 'STATION_ID')\n",
    "\n",
    "# Fill the easy start stations with min and max date\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')) \\\n",
    "    .withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date')) # 1000 days from the start date\n",
    "\n",
    "# Expand to add all days\n",
    "weather_df = weather_df \\\n",
    "    .withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))) \\\n",
    "    .drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# Left join to make sure all dates are there (missing dates will get null)\n",
    "weather_df = weather_df \\\n",
    "    .join(filtered_union, [\"STATION_ID\", \"DATE\"], \"left\") \\\n",
    "    .sort(['STATION_ID', 'DATE'])\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"TEMP\" , weather_df.TEMP.cast('float')/10.0)\n",
    "\n",
    "weather_df = weather_df.replace([999, 9999], None)\n",
    "\n",
    "window_ff = Window.partitionBy('STATION_ID')\\\n",
    "               .orderBy('DATE')\\\n",
    "               .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "window_bf = Window.partitionBy('STATION_ID')\\\n",
    "               .orderBy('DATE')\\\n",
    "               .rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "               \n",
    "\n",
    "def interpol_middle(y, y_prev, y_next):\n",
    "    if y_prev is None and y_next is not None:\n",
    "        return y_next\n",
    "    elif y_next is None and y_prev is not None:\n",
    "        return y_prev\n",
    "    elif y_next is None and y_prev is None:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return (y_prev + y_next) / 2.0\n",
    "\n",
    "#Forward and backwards filling, takes around 6 minutes to complete\n",
    "read_last = f.last(weather_df['TEMP'], ignorenulls=True).over(window_ff)\n",
    "read_next = f.first(weather_df['TEMP'], ignorenulls=True).over(window_bf)\n",
    "\n",
    "# add the columns to the dataframe\n",
    "df_filled = weather_df.withColumn('readvalue_ff', read_last.cast('float'))\\\n",
    "                        .withColumn('readvalue_bf', read_next.cast('float'))\n",
    "            \n",
    "interpol_udf = f.udf(interpol, FloatType())   \n",
    "interpol_middle_udf = f.udf(interpol_middle, FloatType())\n",
    "## Set Date To Unix Time\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "df_filled = df_filled.withColumn('unix', unix_timestamp('DATE'))\n",
    "\n",
    "\n",
    "# df_filled = df_filled.withColumn('TEMP_INTERPOL', interpol_udf('unix', 'readtime_ff', 'readtime_bf', 'readvalue_ff', 'readvalue_bf', 'TEMP'))\\\n",
    "#                     .drop('readtime_ff', 'readtime_bf')\\\n",
    "\n",
    "df_filled = df_filled.withColumn('TEMP', interpol_middle_udf('TEMP','readvalue_ff', 'readvalue_bf'))\\\n",
    "                    .drop('readtime_ff', 'readtime_bf', 'unix', 'readvalue_ff', 'readvalue_bf')\\\n",
    "\n",
    "# UNCOMMENT\n",
    "df_filled.persist()\n",
    "\n",
    "# df_filled.schema\n",
    "df_filled.limit(10).show()\n",
    "# print(f\"Number of rows: {weather_df.count()}\")\n",
    "df_filled.filter(df_filled.TEMP.isNull()).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.coalesce(1).write.csv('WEATHER_DATA_IMPUTED7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+\n",
      "|               STOCK|      DATE| Price|\n",
      "+--------------------+----------+------+\n",
      "|19272.Europa_Deut...|2017-01-01| 6.072|\n",
      "|19272.Europa_Deut...|2017-01-02| 6.072|\n",
      "|19272.Europa_Deut...|2017-01-03| 6.115|\n",
      "|19272.Europa_Deut...|2017-01-04| 6.111|\n",
      "|19272.Europa_Deut...|2017-01-05| 6.348|\n",
      "|19272.Europa_Deut...|2017-01-06| 6.295|\n",
      "|19272.Europa_Deut...|2017-01-07|6.3005|\n",
      "|19272.Europa_Deut...|2017-01-08|6.3005|\n",
      "|19272.Europa_Deut...|2017-01-09| 6.306|\n",
      "|19272.Europa_Deut...|2017-01-10| 6.292|\n",
      "|19272.Europa_Deut...|2017-01-11| 6.296|\n",
      "|19272.Europa_Deut...|2017-01-12| 6.291|\n",
      "|19272.Europa_Deut...|2017-01-13| 6.313|\n",
      "|19272.Europa_Deut...|2017-01-14|6.2995|\n",
      "|19272.Europa_Deut...|2017-01-15|6.2995|\n",
      "|19272.Europa_Deut...|2017-01-16| 6.286|\n",
      "|19272.Europa_Deut...|2017-01-17| 6.292|\n",
      "|19272.Europa_Deut...|2017-01-18| 6.301|\n",
      "|19272.Europa_Deut...|2017-01-19|  6.31|\n",
      "|19272.Europa_Deut...|2017-01-20| 6.155|\n",
      "+--------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 1095000\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "stocks_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .csv(stocks_dataset_path)\n",
    "\n",
    "# Rename columns and drop volume\n",
    "stocks_df = stocks_df.selectExpr(\n",
    "    '_c0 AS STOCK',\n",
    "    '_c1 AS DATE',\n",
    "    '_c2 AS PRICE',\n",
    "    '_c3 AS VOLUME'\n",
    ")\n",
    "stocks_df = stocks_df.drop(\"VOLUME\")\n",
    "\n",
    "# Select 1000 stocks with appropriate years\n",
    "df_stocks_selection = stocks_df \\\n",
    "    .filter((stocks_df.DATE.contains(\"2017\") | stocks_df.DATE.contains(\"2018\") | stocks_df.DATE.contains(\"2019\")))\n",
    "df_stocks_selection_filtered = df_stocks_selection \\\n",
    "    .filter(df_stocks_selection.DATE == \"01/02/2017\") \\\n",
    "    .select('STOCK') \\\n",
    "    .limit(number_stocks_and_weather)\n",
    "\n",
    "selected_stocks = df_stocks_selection_filtered \\\n",
    "    .join(df_stocks_selection, 'STOCK')\n",
    "\n",
    "# \n",
    "stocks_df = df_stocks_selection_filtered \\\n",
    "    .withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')) \\\n",
    "    .withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "stocks_df = stocks_df \\\n",
    "    .withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))) \\\n",
    "    .drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# \n",
    "modifiedDF = selected_stocks \\\n",
    "    .withColumn(\"DATE\", f.to_date(\"DATE\", \"MM/dd/yyyy\")) \\\n",
    "    .dropDuplicates([\"STOCK\", \"DATE\"])\n",
    "stocks_df = stocks_df \\\n",
    "    .join(modifiedDF, [\"Stock\", \"DATE\"], \"left\") \\\n",
    "    .sort(['STOCK', 'DATE'])\\\n",
    "\n",
    "#stocks_df = stocks_df.withColumn(\"PRICE\", stocks_df.PRICE.cast(\"int\"))\n",
    "\n",
    "#Forward and backwards filling, takes around 6 minutes to complete\n",
    "w_forward = Window.partitionBy('Stock').orderBy('Date').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "w_backward = Window.partitionBy('Stock').orderBy('Date').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "\n",
    "#Forward and backwards filling, takes around 6 minutes to complete\n",
    "read_last_price = f.last(stocks_df['Price'], ignorenulls=True).over(w_forward)\n",
    "#read_last_volume = f.last(stocks_df['Volume'], ignorenulls=True).over(w_forward)\n",
    "\n",
    "read_next_price = f.first(stocks_df['Price'], ignorenulls=True).over(w_backward)\n",
    "#read_next_volume = f.first(stocks_df['Volume'], ignorenulls=True).over(window_bf)\n",
    "\n",
    "# add the columns to the dataframe\n",
    "df_filled = stocks_df.withColumn('read_last_price', read_last_price.cast('float'))\\\n",
    "                        .withColumn('read_next_price', read_next_price.cast('float'))\\\n",
    "                       # .withColumn('read_last_volume', read_last_volume.cast('float'))\\\n",
    "                       # .withColumn('read_next_volume', read_next_volume.cast('float'))\n",
    "\n",
    "df_filled = df_filled.withColumn('Price',interpol_middle_udf('Price','read_last_price', 'read_next_price'))\\\n",
    "                             .drop('read_last_price', 'read_next_price')\n",
    "\n",
    "\n",
    "df_filled.persist()\n",
    "\n",
    "df_filled.show()\n",
    "print(f\"Number of rows: {df_filled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|STOCK|DATE|Price|\n",
      "+-----+----+-----+\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filled.filter(df_filled.Price.isNull()).limit(10).show()\n",
    "df_filled.coalesce(1).write.csv('STOCKS_DATA_IMPUTED5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SQL query used:\n",
    "\n",
    "SELECT sum(X*Y') / (sqrt(sum(X))*sqrt(sum(Y')))\n",
    "FROM (\n",
    "    SELECT S.DATE, S.STOCK, W.STATION_ID_1, W.STATION_ID_2, S.PRICE as X, W.Y'\n",
    "    FROM stocks AS S, (\n",
    "        SELECT W1.DATE, W1.STATION_ID AS STATION_ID_1, W2.STATION_ID AS STATION_ID_2, avg|max|min(W1.TEMP, W2.TEMP) as W.Y'\n",
    "        FROM weather AS W1, weather AS W2,\n",
    "        WHERE W1.DATE = W2.DATE\n",
    "    ) AS W\n",
    "    WHERE S.DATE = W.DATE\n",
    ")\n",
    "GROUPBY STOCK, STATION_ID_1, STATION_ID_2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+------+\n",
      "|      DATE|STATION_ID_1|STATION_ID_2|    Y'|\n",
      "+----------+------------+------------+------+\n",
      "|2017-01-01| 02265099999| 94804099999|  85.0|\n",
      "|2017-01-01| 02265099999| 94677099999|  95.5|\n",
      "|2017-01-01| 02265099999| 94584099999| 156.5|\n",
      "|2017-01-01| 02265099999| 83650099999| 127.5|\n",
      "|2017-01-01| 02265099999| 76749399999| 137.5|\n",
      "|2017-01-01| 02265099999| 71889099999| -22.0|\n",
      "|2017-01-01| 02265099999| 71453099999| -20.0|\n",
      "|2017-01-01| 02265099999| 71450099999| -37.0|\n",
      "|2017-01-01| 02265099999| 71322099999|-171.5|\n",
      "|2017-01-01| 02265099999| 70333325518|  31.0|\n",
      "|2017-01-01| 02265099999| 70259526559| -27.5|\n",
      "|2017-01-01| 02265099999| 68487099999|  90.0|\n",
      "|2017-01-01| 02265099999| 63708099999| 112.5|\n",
      "|2017-01-01| 02265099999| 62398099999|  37.5|\n",
      "|2017-01-01| 02265099999| 54292099999| -76.0|\n",
      "|2017-01-01| 02265099999| 43278099999| 115.5|\n",
      "|2017-01-01| 02265099999| 41862199999|  63.0|\n",
      "|2017-01-01| 02265099999| 28552099999| -17.0|\n",
      "|2017-01-01| 02265099999| 06022499999|  32.5|\n",
      "|2017-01-01| 02265099999| 02265099999|  -5.0|\n",
      "+----------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 438000\n"
     ]
    }
   ],
   "source": [
    "# agg_function = f.greatest\n",
    "# agg_function = f.least\n",
    "agg_function = lambda col1, col2: (col(col1) + col(col2)) / 2\n",
    "\n",
    "Y_df = weather_df \\\n",
    "    .withColumnRenamed(\"STATION_ID\", \"STATION_ID_1\") \\\n",
    "    .withColumnRenamed(\"TEMP\", \"TEMP_1\") \\\n",
    "    .join(\n",
    "        weather_df \\\n",
    "            .withColumnRenamed(\"STATION_ID\", \"STATION_ID_2\") \\\n",
    "            .withColumnRenamed(\"TEMP\", \"TEMP_2\"), \n",
    "        \"DATE\"\n",
    "    )\n",
    "Y_df = Y_df.withColumn(\"Y'\", agg_function(\"TEMP_1\", \"TEMP_2\"))\n",
    "Y_df = Y_df.drop(\"TEMP_1\", \"TEMP_2\")\n",
    "Y_df.persist()\n",
    "Y_df.show()\n",
    "print(f\"Number of rows: {Y_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+-------------------+\n",
      "|               STOCK|STATION_ID_1|STATION_ID_2|         COSINE_SIM|\n",
      "+--------------------+------------+------------+-------------------+\n",
      "|42814.Nordamerika...| 02265099999| 94677099999| 0.8642378548973022|\n",
      "|4438.Asien--Austr...| 02265099999| 76749399999| 0.7725067131938536|\n",
      "|43297.Nordamerika...| 02265099999| 43278099999| 0.8487977948316912|\n",
      "|13784.Europa_Deut...| 06022499999| 94677099999| 0.5193424493119109|\n",
      "|28549.Futures--In...| 28552099999| 94804099999| 0.7120823821302918|\n",
      "|32843.Nordamerika...| 28552099999| 62398099999| 0.6929047123707867|\n",
      "|31474.Nordamerika...| 28552099999| 54292099999|0.11055468034407347|\n",
      "|41574.Nordamerika...| 41862199999| 71450099999|0.45457949948720755|\n",
      "|34271.Nordamerika...| 43278099999| 94584099999| 0.5726049863036856|\n",
      "|13784.Europa_Deut...| 43278099999| 71453099999| 0.4919507413036301|\n",
      "|38565.Nordamerika...| 62398099999| 71450099999| 0.6591657325739723|\n",
      "|37239.Nordamerika...| 68487099999| 02265099999| 0.7869850544965445|\n",
      "|4438.Asien--Austr...| 70259526559| 43278099999| 0.7717232453680575|\n",
      "|13784.Europa_Deut...| 70259526559| 41862199999| 0.4370994506051258|\n",
      "|40528.Nordamerika...| 70333325518| 94804099999| 0.5286089090038335|\n",
      "|4438.Asien--Austr...| 70333325518| 71450099999| 0.4419847306754703|\n",
      "|41574.Nordamerika...| 70333325518| 06022499999| 0.5116370945591366|\n",
      "|40702.Nordamerika...| 70333325518| 06022499999| 0.8788932729977597|\n",
      "|13784.Europa_Deut...| 71322099999| 83650099999| 0.3997707207427156|\n",
      "|28721.Futures--In...| 71322099999| 71889099999|0.16830833044206067|\n",
      "+--------------------+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows: 8000\n"
     ]
    }
   ],
   "source": [
    "combined_df = stocks_df \\\n",
    "    .withColumnRenamed(\"PRICE\", \"X\") \\\n",
    "    .join(Y_df, \"DATE\")\n",
    "\n",
    "combined_df_grouped = combined_df \\\n",
    "    .groupBy(\"STOCK\", \"STATION_ID_1\", \"STATION_ID_2\") \\\n",
    "    .agg(\n",
    "        f.sqrt(f.sum(f.pow(\"X\", 2))).alias(\"X_norm\"),\n",
    "        f.sqrt(f.sum(f.pow(\"Y'\", 2))).alias(\"Y_norm\"),\n",
    "        f.sum(col(\"X\") * col(\"Y'\")).alias(\"XY\")\n",
    "    )\n",
    "\n",
    "combined_df_grouped = combined_df_grouped \\\n",
    "    .withColumn(\"COSINE_SIM\", col(\"XY\") / (col(\"Y_norm\") * col(\"X_norm\")))\n",
    "combined_df_grouped = combined_df_grouped.drop(\"X_norm\", \"Y_norm\", \"XY\")\n",
    "combined_df_grouped.persist()\n",
    "combined_df_grouped.show()\n",
    "\n",
    "print(f\"Number of rows: {combined_df_grouped.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stocks_df.filter(col(\"PRICE\").isNotNull()).count()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3057157393a05f1e3a652ff8b92c3f8767e072f6c4cd1c84e087da81568d2ff"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
