{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x16e3b220d30>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-4KLPD8H:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>cas2</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"cas2\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext \n",
    "\n",
    "#!!! MAKE SURE THAT YOUR TEXTFILE HEADER IS THIS EXACT STRING: !!!\n",
    "\"STATION_ID,DATE,LOCATION,WIND,TMP,DEW,SLP\"\n",
    "\n",
    "weather_path_2017 = 'out_2017.txt'\n",
    "weather_path_2018 = 'out_2018.txt'\n",
    "weather_path_2019 = 'out_2019.txt'\n",
    "weather_path_2020 = 'out_2020.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType\n",
    "schema = StructType([StructField('STATION_ID', StringType(), False),\\\n",
    "                    StructField('DATE', StringType(), False),\\\n",
    "                    StructField('LOCATION', StringType(), False),\\\n",
    "                    StructField('WIND', StringType(), False),\\\n",
    "                    StructField('TMP', StringType(), False),\\\n",
    "                    StructField('DEW', StringType(), False),\\\n",
    "                    StructField('SLP', StringType(), False)])\n",
    "\n",
    "df_2017 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2017)\n",
    "df_2017 = df_2017.withColumn(\"splitcount\", f.size(f.split(df_2017.WIND, \" \")))\n",
    "df_2017 = df_2017.filter(\"splitcount == 5\")\n",
    "df_2017 = df_2017.withColumn('wind_angle', f.split(df_2017['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2017['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2017['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2017['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2017['WIND'], ' ').getItem(4))\n",
    "df_2017 = df_2017.withColumn('tmp_val', f.split(df_2017['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2017['TMP'], ' ').getItem(1))\n",
    "df_2017 = df_2017.withColumn('dew_val', f.split(df_2017['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2017['DEW'], ' ').getItem(1))\n",
    "df_2017 = df_2017.withColumn('slp_val', f.split(df_2017['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2017['SLP'], ' ').getItem(1))\n",
    "df_2017 = df_2017.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\", \"wind_qualityAngle\", \"wind_type\", \"wind_qualitySpeed\", \"tmp_quality\", \"dew_quality\", \"slp_quality\")\n",
    "\n",
    "df_2018 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2018)\n",
    "df_2018 = df_2018.withColumn(\"splitcount\", f.size(f.split(df_2018.WIND, \" \")))\n",
    "df_2018 = df_2018.filter(\"splitcount == 5\")\n",
    "df_2018 = df_2018.withColumn('wind_angle', f.split(df_2018['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2018['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2018['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2018['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2018['WIND'], ' ').getItem(4))\n",
    "df_2018 = df_2018.withColumn('tmp_val', f.split(df_2018['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2018['TMP'], ' ').getItem(1))\n",
    "df_2018 = df_2018.withColumn('dew_val', f.split(df_2018['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2018['DEW'], ' ').getItem(1))\n",
    "df_2018 = df_2018.withColumn('slp_val', f.split(df_2018['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2018['SLP'], ' ').getItem(1))\n",
    "df_2018 = df_2018.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\", \"wind_qualityAngle\", \"wind_type\", \"wind_qualitySpeed\", \"tmp_quality\", \"dew_quality\", \"slp_quality\")\n",
    "\n",
    "df_2019 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2019)\n",
    "df_2019 = df_2019.withColumn(\"splitcount\", f.size(f.split(df_2019.WIND, \" \")))\n",
    "df_2019 = df_2019.filter(\"splitcount == 5\")\n",
    "df_2019 = df_2019.withColumn('wind_angle', f.split(df_2019['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2019['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2019['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2019['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2019['WIND'], ' ').getItem(4))\n",
    "df_2019 = df_2019.withColumn('tmp_val', f.split(df_2019['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2019['TMP'], ' ').getItem(1))\n",
    "df_2019 = df_2019.withColumn('dew_val', f.split(df_2019['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2019['DEW'], ' ').getItem(1))\n",
    "df_2019 = df_2019.withColumn('slp_val', f.split(df_2019['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2019['SLP'], ' ').getItem(1))\n",
    "df_2019 = df_2019.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\", \"wind_qualityAngle\", \"wind_type\", \"wind_qualitySpeed\", \"tmp_quality\", \"dew_quality\", \"slp_quality\")\n",
    "\n",
    "union = df_2017.union(df_2018).union(df_2019).sort(['STATION_ID', 'DATE'])\n",
    "\n",
    "# select stations which easy start date and then first 1000 of those\n",
    "df_union_selection = union.filter(union.DATE == \"2017-01-01\").select('STATION_ID').limit(1000)\n",
    "\n",
    "# filter these over the complete dataset\n",
    "filtered_union= df_union_selection.join(union, 'STATION_ID')\n",
    "\n",
    "\n",
    "# fill the easy start stations with min and max date\n",
    "df_union_selection = df_union_selection.withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "\n",
    "# expand to add all days\n",
    "df_union_selection = df_union_selection.withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# left join to make sure all dates are there (missing dates will get null)\n",
    "df_union_selection = df_union_selection.join(filtered_union, [\"STATION_ID\", \"DATE\"], \"left\").sort(['STATION_ID', 'DATE'])\n",
    "df_union_selection = df_union_selection.withColumn(\"wind_speed\", df_union_selection.wind_speed.cast('int')).withColumn(\"wind_angle\", df_union_selection.wind_angle.cast('int')).withColumn(\"tmp_val\", df_union_selection.tmp_val.cast('int')).withColumn(\"dew_val\", df_union_selection.dew_val.cast('int')).withColumn(\"slp_val\", df_union_selection.slp_val.cast('int'))\n",
    "df_union_selection = df_union_selection.replace([999, 9999], None)\n",
    "\n",
    "# if you want to see the first 5000\n",
    "# print(df_union_selection.show(5000))\n",
    "\n",
    "# work in progress for filling\n",
    "# w_forward = Window.partitionBy().orderBy('STATION_ID').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "# w_backward = Window.partitionBy().orderBy('STATION_ID').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "# df_2017_selection = df_2017_selection.withColumn('wind_angle',f.last('wind_angle',ignorenulls=True).over(w_forward)).withColumn('wind_angle',f.first('wind_angle',ignorenulls=True).over(w_backward))\n",
    "# df_2017_selection = df_2017_selection.withColumn('wind_speed',f.last('wind_speed',ignorenulls=True).over(w_forward)).withColumn('wind_speed',f.first('wind_speed',ignorenulls=True).over(w_backward))\n",
    "# df_2017_selection = df_2017_selection.withColumn('tmp_val',f.last('tmp_val',ignorenulls=True).over(w_forward)).withColumn('tmp_val',f.first('tmp_val',ignorenulls=True).over(w_backward))\n",
    "# df_2017_selection = df_2017_selection.withColumn('dew_val',f.last('dew_val',ignorenulls=True).over(w_forward)).withColumn('dew_val',f.first('dew_val',ignorenulls=True).over(w_backward))\n",
    "# df_2017_selection = df_2017_selection.withColumn('slp_val',f.last('slp_val',ignorenulls=True).over(w_forward)).withColumn('slp_val',f.first('slp_val',ignorenulls=True).over(w_backward))\n",
    "    # .withColumn('wind_speed',f.last('wind_speed',ignorenulls=True).over(w_forward))\\\n",
    "    # .withColumn('wind_speed',f.first('wind_speed',ignorenulls=True).over(w_backward))\n",
    "\n",
    "\n",
    "\n",
    "# test distribution\n",
    "# print(df_union_selection.groupBy('STATION_ID').count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_date_split = df_good_cols.withColumn('year', f.split(df_good_cols['DATE'], '-').getItem(0)).withColumn('month', f.split(df_good_cols['DATE'], '-').getItem(1)).withColumn('day', f.split(df_good_cols['DATE'], '-').getItem(2))\n",
    "# df_date_split = df_date_split.drop(\"DATE\")\n",
    "# df_date_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, avg\n",
    "# df_date_split.select(avg('month')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+------+\n",
      "|               Stock|      Date|Price|Volume|\n",
      "+--------------------+----------+-----+------+\n",
      "|100.Asien--Austra...|2017-01-01| null|  null|\n",
      "|100.Asien--Austra...|2017-01-02| 10.6|  1518|\n",
      "|100.Asien--Austra...|2017-01-03| 10.6|  3444|\n",
      "|100.Asien--Austra...|2017-01-04|10.65|  8253|\n",
      "|100.Asien--Austra...|2017-01-05|10.87| 10861|\n",
      "|100.Asien--Austra...|2017-01-06|11.03|  3622|\n",
      "|100.Asien--Austra...|2017-01-07| null|  null|\n",
      "|100.Asien--Austra...|2017-01-08| null|  null|\n",
      "|100.Asien--Austra...|2017-01-09|10.87|   150|\n",
      "|100.Asien--Austra...|2017-01-10|10.87|  1387|\n",
      "|100.Asien--Austra...|2017-01-11|10.87|  8785|\n",
      "|100.Asien--Austra...|2017-01-12|11.03| 30199|\n",
      "|100.Asien--Austra...|2017-01-13|   11|  1098|\n",
      "|100.Asien--Austra...|2017-01-14| null|  null|\n",
      "|100.Asien--Austra...|2017-01-15| null|  null|\n",
      "|100.Asien--Austra...|2017-01-16|10.87|  8780|\n",
      "|100.Asien--Austra...|2017-01-17| 10.5|  2109|\n",
      "|100.Asien--Austra...|2017-01-18|10.55|  9613|\n",
      "|100.Asien--Austra...|2017-01-19|10.41| 18914|\n",
      "|100.Asien--Austra...|2017-01-20|10.65|  7855|\n",
      "+--------------------+----------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType\n",
    "stocks_file = \"Stocks.txt\"\n",
    "df_stocks = spark.read.option(\"header\", \"false\").csv(stocks_file)\n",
    "df_stocks =  df_stocks.selectExpr(\n",
    "    '_c0 AS Stock',\n",
    "    '_c1 AS Date',\n",
    "    '_c2 AS Price',\n",
    "    '_c3 AS Volume',\n",
    ")\n",
    "\n",
    "#\n",
    "df_stocks_selection = df_stocks.filter((df_stocks.Date.contains(\"2017\") | df_stocks.Date.contains(\"2018\") | df_stocks.Date.contains(\"2019\")))\n",
    "df_stocks_selection_filtered = df_stocks_selection.filter(df_stocks_selection.Date == \"01/02/2017\").select('Stock').limit(1000)\n",
    "selected_stocks = df_stocks_selection_filtered.join(df_stocks_selection, 'Stock')\n",
    "#\n",
    "stocks_dates = df_stocks_selection_filtered.withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "stocks_dates = stocks_dates.withColumn('Date', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "\n",
    "modifiedDF = selected_stocks.withColumn(\"Date\", f.to_date(\"Date\", \"MM/dd/yyyy\")).dropDuplicates([\"Stock\", \"Date\"])\n",
    "stocks_dates = stocks_dates.join(modifiedDF, [\"Stock\", \"Date\"], \"left\").sort(['Stock', 'Date'])\n",
    "\n",
    "# w_forward = Window.partitionBy().orderBy('Stock').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "# w_backward = Window.partitionBy().orderBy('Stock').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "# stocks_dates = stocks_dates.withColumn('Price',f.last('Price',ignorenulls=True).over(w_forward)).withColumn('Price',f.first('Price',ignorenulls=True).over(w_backward))\n",
    "# w_forward2 = Window.partitionBy().orderBy('Stock').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "# w_backward2 = Window.partitionBy().orderBy('Stock').rowsBetween(Window.currentRow,Window.unboundedFollowing)\n",
    "# stocks_dates = stocks_dates.withColumn('Volume',f.last('Volume',ignorenulls=True).over(w_forward2)).withColumn('Volume',f.first('Volume',ignorenulls=True).over(w_backward2))\n",
    "print(stocks_dates.show())\n",
    "# df_stocks = df_stocks.withColumn(\"Price\",df_stocks.Price.cast('float'))\n",
    "# df_stocks = df_stocks.withColumn(\"Volume\",df_stocks.Volume.cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+------+-----+---+----+\n",
      "|               Stock|      Date|Price|Volume|month|day|year|\n",
      "+--------------------+----------+-----+------+-----+---+----+\n",
      "|32843.Nordamerika...|01/01/2016|18.98| 50635|    1|  1|2016|\n",
      "|32843.Nordamerika...|01/04/2016|18.52| 51616|    1|  4|2016|\n",
      "|32843.Nordamerika...|01/05/2016|19.15| 54898|    1|  5|2016|\n",
      "|32843.Nordamerika...|01/06/2016|19.71| 41555|    1|  6|2016|\n",
      "|32843.Nordamerika...|01/07/2016|19.17| 44430|    1|  7|2016|\n",
      "|32843.Nordamerika...|01/08/2016|18.94| 72673|    1|  8|2016|\n",
      "|32843.Nordamerika...|01/11/2016| 19.1| 45426|    1| 11|2016|\n",
      "|32843.Nordamerika...|01/12/2016|19.39| 61457|    1| 12|2016|\n",
      "|32843.Nordamerika...|01/13/2016|19.27| 61805|    1| 13|2016|\n",
      "|32843.Nordamerika...|01/14/2016|19.17| 35597|    1| 14|2016|\n",
      "|32843.Nordamerika...|01/15/2016|18.81| 69227|    1| 15|2016|\n",
      "|32843.Nordamerika...|01/18/2016|18.81| 69227|    1| 18|2016|\n",
      "|32843.Nordamerika...|01/19/2016|18.82| 23700|    1| 19|2016|\n",
      "|32843.Nordamerika...|01/20/2016|17.97| 41439|    1| 20|2016|\n",
      "|32843.Nordamerika...|01/21/2016|17.82| 35240|    1| 21|2016|\n",
      "|32843.Nordamerika...|01/22/2016|18.62| 79139|    1| 22|2016|\n",
      "|32843.Nordamerika...|01/25/2016|18.09| 89251|    1| 25|2016|\n",
      "|32843.Nordamerika...|01/26/2016|17.62| 75300|    1| 26|2016|\n",
      "|32843.Nordamerika...|01/27/2016|17.58| 43142|    1| 27|2016|\n",
      "|32843.Nordamerika...|01/28/2016| 17.7| 20499|    1| 28|2016|\n",
      "+--------------------+----------+-----+------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks_dates = df_stocks.withColumn('month', f.split(df_stocks['Date'], '/').getItem(0).cast('int')).withColumn('day', f.split(df_stocks['Date'], '/').getItem(1).cast('int')).withColumn('year', f.split(df_stocks['Date'], '/').getItem(2).cast('int'))\n",
    "# df_stocks_dates = df_stocks_dates.drop(\"Date\")\n",
    "df_stocks_dates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Stock: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks_dates.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjmar\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock1 = \"32843.Nordamerika_USA-NASDAQ_CRA-International-Inc._CRAI\"\n",
    "stock2 = \"41574.Nordamerika_USA-OTC_Polydex-Pharmaceuticals_POLXF\"\n",
    "stock3 = \"23349.Europa_Schweden_BlackPearl-Resources-Inc.-Reg.-Shares-SDRs-1-o._02155Z\"\n",
    "df_stock_1 = df_stocks_dates.filter(f\"Stock = '{stock1}'\").limit(10).registerTempTable(\"stock1\")\n",
    "df_stock_2 = df_stocks_dates.filter(f\"Stock = '{stock2}'\").limit(10).registerTempTable(\"stock2\")\n",
    "df_stock_3 = df_stocks_dates.filter(f\"Stock = '{stock3}'\").limit(10).registerTempTable(\"stock3\")\n",
    "\n",
    "# spark.sql(\"CACHE TABLE testCache1 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock1\")\n",
    "# spark.sql(\"CACHE TABLE testCache2 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock2\")\n",
    "# spark.sql(\"CACHE TABLE testCache3 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock3\")\n",
    "\n",
    "spark.sql(\"CACHE TABLE stock1\")\n",
    "spark.sql(\"CACHE TABLE stock2\")\n",
    "spark.sql(\"CACHE TABLE stock3\")\n",
    "\n",
    "\n",
    "# UNCOMMENT IF FIRST TIME\n",
    "spark.sql(\\\n",
    "  \"CREATE TEMP VIEW COLS3 AS SELECT \\\n",
    "  stock1.volume X, stock2.volume Y1, stock3.volume Y2 FROM stock1\\\n",
    "  INNER JOIN stock2 ON stock1.day = stock2.day\\\n",
    "   INNER JOIN stock3 ON stock1.Date = stock3.Date\"\\\n",
    "     ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|    X|   Y1|    Y2|\n",
      "+-----+-----+------+\n",
      "|50635|12000|200784|\n",
      "|51616|  400|170002|\n",
      "|54898|  100| 66159|\n",
      "|41555| 3200| 66159|\n",
      "|44430| 4976|387422|\n",
      "|72673| 4976|121141|\n",
      "|45426|  100| 90265|\n",
      "|61457|  100|128003|\n",
      "|61805|  100| 51492|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+------+\n",
      "|    X|   Y1|    Y2|Y_star|\n",
      "+-----+-----+------+------+\n",
      "|50635|12000|200784|   TWO|\n",
      "|51616|  400|170002|   ONE|\n",
      "|54898|  100| 66159|   TWO|\n",
      "|41555| 3200| 66159|   TWO|\n",
      "|44430| 4976|387422|   ONE|\n",
      "|72673| 4976|121141|   ONE|\n",
      "|45426|  100| 90265|   TWO|\n",
      "|61457|  100|128003|   TWO|\n",
      "|61805|  100| 51492|   TWO|\n",
      "+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM COLS3\").show()\n",
    "\n",
    "spark.sql(\"SELECT *, CASE WHEN Y1 >= Y2 THEN 'ONE' WHEN Y2 > Y1 THEN 'TWO' END AS Y_star FROM COLS3\").show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a95df2450c988ecaa48156885f40f8f4d0aecca6e0393df2e32f71b5b0bf7d9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}