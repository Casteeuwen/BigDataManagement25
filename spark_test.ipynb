{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1d87fde8a00>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-4KLPD8H:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>cas2</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"cas2\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext \n",
    "\n",
    "#!!! MAKE SURE THAT YOUR TEXTFILE HEADER IS THIS EXACT STRING: !!!\n",
    "\"STATION_ID,DATE,LOCATION,WIND,TMP,DEW,SLP\"\n",
    "\n",
    "weather_path_2017 = 'out_2017.txt'\n",
    "weather_path_2018 = 'out_2018.txt'\n",
    "weather_path_2019 = 'out_2019.txt'\n",
    "weather_path_2020 = 'out_2020.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION_ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- WIND: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      "\n",
      "None\n",
      "1000 1000 1000\n",
      "365000 365000 365000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType\n",
    "# schema = StructType([StructField('STATION_ID', StringType(), False),\\\n",
    "#                     StructField('DATE', DateType(), False),\\\n",
    "#                     StructField('LOCATION', StringType(), False),\\\n",
    "#                     StructField('WIND', StringType(), False),\\\n",
    "#                     StructField('TMP', StringType(), False),\\\n",
    "#                     StructField('DEW', StringType(), False),\\\n",
    "#                     StructField('SLP', StringType(), False)])\n",
    "\n",
    "df_2017 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2017)\n",
    "print(df_2017.printSchema())\n",
    "df_2017 = df_2017.withColumn(\"splitcount\", f.size(f.split(df_2017.WIND, \" \")))\n",
    "df_2017 = df_2017.filter(\"splitcount == 5\")\n",
    "df_2017 = df_2017.withColumn('wind_angle', f.split(df_2017['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2017['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2017['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2017['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2017['WIND'], ' ').getItem(4))\n",
    "df_2017 = df_2017.withColumn('tmp_val', f.split(df_2017['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2017['TMP'], ' ').getItem(1))\n",
    "df_2017 = df_2017.withColumn('dew_val', f.split(df_2017['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2017['DEW'], ' ').getItem(1))\n",
    "df_2017 = df_2017.withColumn('slp_val', f.split(df_2017['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2017['SLP'], ' ').getItem(1))\n",
    "df_2017 = df_2017.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\")\n",
    "\n",
    "df_2018 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2018)\n",
    "df_2018 = df_2018.withColumn(\"splitcount\", f.size(f.split(df_2018.WIND, \" \")))\n",
    "df_2018 = df_2018.filter(\"splitcount == 5\")\n",
    "df_2018 = df_2018.withColumn('wind_angle', f.split(df_2018['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2018['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2018['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2018['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2018['WIND'], ' ').getItem(4))\n",
    "df_2018 = df_2018.withColumn('tmp_val', f.split(df_2018['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2018['TMP'], ' ').getItem(1))\n",
    "df_2018 = df_2018.withColumn('dew_val', f.split(df_2018['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2018['DEW'], ' ').getItem(1))\n",
    "df_2018 = df_2018.withColumn('slp_val', f.split(df_2018['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2018['SLP'], ' ').getItem(1))\n",
    "df_2018 = df_2018.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\")\n",
    "\n",
    "df_2019 = spark.read.option(\"header\", \"true\").option(\"dateFormat\", \"yyyy-mm-dd\").csv(weather_path_2019)\n",
    "df_2019 = df_2019.withColumn(\"splitcount\", f.size(f.split(df_2019.WIND, \" \")))\n",
    "df_2019 = df_2019.filter(\"splitcount == 5\")\n",
    "df_2019 = df_2019.withColumn('wind_angle', f.split(df_2019['WIND'], ' ').getItem(0)).withColumn('wind_qualityAngle', f.split(df_2019['WIND'], ' ').getItem(1)).withColumn('wind_type', f.split(df_2019['WIND'], ' ').getItem(2)).withColumn('wind_speed', f.split(df_2019['WIND'], ' ').getItem(3)).withColumn('wind_qualitySpeed', f.split(df_2019['WIND'], ' ').getItem(4))\n",
    "df_2019 = df_2019.withColumn('tmp_val', f.split(df_2019['TMP'], ' ').getItem(0)).withColumn('tmp_quality', f.split(df_2019['TMP'], ' ').getItem(1))\n",
    "df_2019 = df_2019.withColumn('dew_val', f.split(df_2019['DEW'], ' ').getItem(0)).withColumn('dew_quality', f.split(df_2019['DEW'], ' ').getItem(1))\n",
    "df_2019 = df_2019.withColumn('slp_val', f.split(df_2019['SLP'], ' ').getItem(0)).withColumn('slp_quality', f.split(df_2019['SLP'], ' ').getItem(1))\n",
    "df_2019 = df_2019.drop(\"splitcount\", \"WIND\", \"TMP\", \"DEW\", \"SLP\")\n",
    "\n",
    "# select stations which easy start date and then first 1000 of those\n",
    "df_2017_selection = df_2017.filter(df_2017.DATE == \"2017-01-01\").select('STATION_ID').limit(1000)\n",
    "df_2018_selection = df_2018.filter(df_2018.DATE == \"2018-01-01\").select('STATION_ID').limit(1000)\n",
    "df_2019_selection = df_2019.filter(df_2019.DATE == \"2019-01-01\").select('STATION_ID').limit(1000)\n",
    "print(df_2017_selection.count(), df_2018_selection.count(), df_2019_selection.count())\n",
    "\n",
    "# filter these over the complete dataset\n",
    "filtered_2017= df_2017_selection.join(df_2017, 'STATION_ID')\n",
    "filtered_2018= df_2018_selection.join(df_2018, 'STATION_ID')\n",
    "filtered_2019= df_2019_selection.join(df_2019, 'STATION_ID')\n",
    "\n",
    "# fill the easy start stations with min and max date\n",
    "df_2017_selection = df_2017_selection.withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2017-12-31\").cast('date'))\n",
    "df_2018_selection = df_2018_selection.withColumn(\"min_date\", f.lit(\"2018-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2018-12-31\").cast('date'))\n",
    "df_2019_selection = df_2019_selection.withColumn(\"min_date\", f.lit(\"2019-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2019-12-31\").cast('date'))\n",
    "\n",
    "# expand to add all days\n",
    "df_2017_selection = df_2017_selection.withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "df_2018_selection = df_2018_selection.withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "df_2019_selection = df_2019_selection.withColumn('DATE', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "\n",
    "# left join to make sure all dates are there (missing dates will get null)\n",
    "df_2017_selection = df_2017_selection.join(filtered_2017, [\"STATION_ID\", \"DATE\"], \"left\")\n",
    "df_2018_selection = df_2018_selection.join(filtered_2018, [\"STATION_ID\", \"DATE\"], \"left\")\n",
    "df_2019_selection = df_2019_selection.join(filtered_2019, [\"STATION_ID\", \"DATE\"], \"left\")\n",
    "print(df_2017_selection.count(), df_2018_selection.count(), df_2019_selection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_date_split = df_good_cols.withColumn('year', f.split(df_good_cols['DATE'], '-').getItem(0)).withColumn('month', f.split(df_good_cols['DATE'], '-').getItem(1)).withColumn('day', f.split(df_good_cols['DATE'], '-').getItem(2))\n",
    "# df_date_split = df_date_split.drop(\"DATE\")\n",
    "# df_date_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, avg\n",
    "# df_date_split.select(avg('month')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+------+\n",
      "|               Stock|      Date|  Price|Volume|\n",
      "+--------------------+----------+-------+------+\n",
      "|32843.Nordamerika...|01/02/2017|  36.62| 29618|\n",
      "|32843.Nordamerika...|01/03/2017|  37.03| 46929|\n",
      "|32843.Nordamerika...|01/04/2017|  36.34| 47533|\n",
      "|32843.Nordamerika...|01/05/2017|  36.49| 29710|\n",
      "|32843.Nordamerika...|01/06/2017|  36.16| 56468|\n",
      "|32843.Nordamerika...|01/09/2017|  34.75| 50080|\n",
      "|32843.Nordamerika...|01/10/2017|  34.57| 34604|\n",
      "|32843.Nordamerika...|01/11/2017|  34.74| 41340|\n",
      "|32843.Nordamerika...|01/12/2017|  35.55| 24844|\n",
      "|32843.Nordamerika...|01/13/2017|   35.3| 39395|\n",
      "|32843.Nordamerika...|01/16/2017|   35.3| 39395|\n",
      "|32843.Nordamerika...|01/17/2017|  35.24| 19939|\n",
      "|32843.Nordamerika...|01/18/2017|33.2543| 30599|\n",
      "|32843.Nordamerika...|01/19/2017|  33.17| 20668|\n",
      "|32843.Nordamerika...|01/20/2017|  33.27| 29281|\n",
      "|32843.Nordamerika...|01/23/2017|   33.5| 42937|\n",
      "|32843.Nordamerika...|01/24/2017|  33.55| 38238|\n",
      "|32843.Nordamerika...|01/25/2017|   34.2| 36908|\n",
      "|32843.Nordamerika...|01/26/2017|   34.4| 23795|\n",
      "|32843.Nordamerika...|01/27/2017|  34.08| 19832|\n",
      "+--------------------+----------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|               Stock|\n",
      "+--------------------+\n",
      "|12862.Diverse_For...|\n",
      "|12831.Diverse_For...|\n",
      "+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType\n",
    "stocks_file = \"Stocks.txt\"\n",
    "df_stocks = spark.read.option(\"header\", \"false\").csv(stocks_file)\n",
    "df_stocks =  df_stocks.selectExpr(\n",
    "    '_c0 AS Stock',\n",
    "    '_c1 AS Date',\n",
    "    '_c2 AS Price',\n",
    "    '_c3 AS Volume',\n",
    ")\n",
    "\n",
    "#\n",
    "df_stocks_selection = df_stocks.filter((df_stocks.Date.contains(\"2017\") | df_stocks.Date.contains(\"2018\") | df_stocks.Date.contains(\"2019\")))\n",
    "print(df_stocks_selection.show())\n",
    "df_stocks_selection_filtered = df_stocks_selection.filter(df_stocks_selection.Date == \"01/01/2017\").select('Stock').limit(1000)\n",
    "print(df_stocks_selection_filtered.show())\n",
    "# selected_stocks = df_stocks_selection_filtered.join(df_stocks_selection, 'Stock')\n",
    "# print(selected_stocks.show(1200))\n",
    "#\n",
    "# stocks_dates = df_stocks_selection_filtered.withColumn(\"min_date\", f.lit(\"2017-01-01\").cast('date')).withColumn(\"max_date\", f.lit(\"2017-12-31\").cast('date'))\n",
    "# stocks_dates = stocks_dates.withColumn('date', f.explode(f.expr('sequence(min_date, max_date, interval 1 day)'))).drop(\"min_date\", \"max_date\")\n",
    "# print(stocks_dates.show(1200))\n",
    "\n",
    "# print(df_stocks_selection.show())\n",
    "\n",
    "\n",
    "# df_stocks = df_stocks.withColumn(\"Price\",df_stocks.Price.cast('float'))\n",
    "# df_stocks = df_stocks.withColumn(\"Volume\",df_stocks.Volume.cast('int'))\n",
    "\n",
    "# df_stocks.show()\n",
    "# df_stocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_dates = df_stocks.withColumn('month', f.split(df_stocks['Date'], '/').getItem(0).cast('int')).withColumn('day', f.split(df_stocks['Date'], '/').getItem(1).cast('int')).withColumn('year', f.split(df_stocks['Date'], '/').getItem(2).cast('int'))\n",
    "# df_stocks_dates = df_stocks_dates.drop(\"Date\")\n",
    "df_stocks_dates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_dates.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock1 = \"32843.Nordamerika_USA-NASDAQ_CRA-International-Inc._CRAI\"\n",
    "stock2 = \"41574.Nordamerika_USA-OTC_Polydex-Pharmaceuticals_POLXF\"\n",
    "stock3 = \"23349.Europa_Schweden_BlackPearl-Resources-Inc.-Reg.-Shares-SDRs-1-o._02155Z\"\n",
    "df_stock_1 = df_stocks_dates.filter(f\"Stock = '{stock1}'\").limit(10).registerTempTable(\"stock1\")\n",
    "df_stock_2 = df_stocks_dates.filter(f\"Stock = '{stock2}'\").limit(10).registerTempTable(\"stock2\")\n",
    "df_stock_3 = df_stocks_dates.filter(f\"Stock = '{stock3}'\").limit(10).registerTempTable(\"stock3\")\n",
    "\n",
    "# spark.sql(\"CACHE TABLE testCache1 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock1\")\n",
    "# spark.sql(\"CACHE TABLE testCache2 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock2\")\n",
    "# spark.sql(\"CACHE TABLE testCache3 OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM stock3\")\n",
    "\n",
    "spark.sql(\"CACHE TABLE stock1\")\n",
    "spark.sql(\"CACHE TABLE stock2\")\n",
    "spark.sql(\"CACHE TABLE stock3\")\n",
    "\n",
    "\n",
    "# UNCOMMENT IF FIRST TIME\n",
    "spark.sql(\\\n",
    "  \"CREATE TEMP VIEW COLS3 AS SELECT \\\n",
    "  stock1.volume X, stock2.volume Y1, stock3.volume Y2 FROM stock1\\\n",
    "  INNER JOIN stock2 ON stock1.day = stock2.day\\\n",
    "   INNER JOIN stock3 ON stock1.Date = stock3.Date\"\\\n",
    "     ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM COLS3\").show()\n",
    "\n",
    "spark.sql(\"SELECT *, CASE WHEN Y1 >= Y2 THEN 'ONE' WHEN Y2 > Y1 THEN 'TWO' END AS Y_star FROM COLS3\").show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a95df2450c988ecaa48156885f40f8f4d0aecca6e0393df2e32f71b5b0bf7d9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}